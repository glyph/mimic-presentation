<!doctype html>
<html lang="en">

  <head>
    <meta charset="utf-8">

    <title>reveal.js - The HTML Presentation Framework</title>

    <meta name="description" content="A framework for easily creating beautiful presentations using HTML">
    <meta name="author" content="Hakim El Hattab">

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <link rel="stylesheet" href="css/reveal.min.css">
    <link rel="stylesheet" href="css/theme/default.css" id="theme">

    <!-- For syntax highlighting -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- If the query includes 'print-pdf', include the PDF print sheet -->
    <script>
      if( window.location.search.match( /print-pdf/gi ) ) {
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = 'css/print/pdf.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
      }
    </script>

    <!--[if lt IE 9]>
	<script src="lib/js/html5shiv.js"></script>
	<![endif]-->
  </head>

  <body>

    <div class="reveal">

      <!-- Any section element inside of this container is displayed as a slide -->
      <div class="slides">
        <section>
          <h1>Mimic</h1>
          <h2>An API Compatible Mock Service For OpenStack</h2>
        </section>

        <section>
          <h1>Introductions</h1>
          <aside class="notes">
            <p>Lekha: Hi, I am Lekha Jeevan, a software developer in Test in Rackspace</p>
            <p>Glyph: and I'm Glyph, also a software developer at Rackspace.</p>
          </aside>
        </section>

        <section>
          <h1>What?</h1>
          <aside class="notes">
            Lekha: Today we are going to talk about Mimic - an opensource
            framework that allows for testing against for OpenStack and
            Rackspace APIs, how it is making testing for products in Rackspace
            a cake walk and how it could potentially do the same for your
            OpenStack-backed applications. Your contributions, will help us get
            there.
          </aside>
        </section>

        <section>
          <h1>Auto Scale</h1>
          <aside class="notes">
            Rackspace Autoscale is a system which automates the process of
            getting the right amount of compute capacity for an application, by
            creating (scaling up) and deleting (scaling down) servers and
            associating them with load balancers.

            In order to perform this task, Autoscale speaks to three back-end
            APIs: Rackspace Identity for authentication and impersonation,
            Rackspace Cloud Servers for provisioning and deleting servers, and
            Rackspace Cloud Load Balancers for adding and removing servers to
            load balancers as they are created and deleted.

            Rackspace Identity is API-compatible with OpenStack Identity v2,
            Rackspace Cloud Servers is powered by and API-compatible with
            OpenStack Compute, although Rackspace Cloud Load Balancers is a
            custom API.
          </aside>
        </section>

        <section>
          <h1>Testing</h1>
          <h2>(for Auto Scale)</h2>
          <aside class="notes">
            The development started, and I was the QE in the group, and I
            started writing automated tests, in parallel to the development
            work. So, as autoscale was interacting with so many other systems,
            testing it dint mean just testing autoscale, but that if any of
            these systems it depended on didn't behave as expected, autoscale
            did not crumble. But was consistent and able to handle them.
          </aside>
        </section>

        <section>
          <h1>Types of Tests</h1>
          <h2>(for Auto Scale)</h2>
          <aside class="notes">
            So, I had two kinds of tests, one was the functional tests, to test
            the API contracts, verify the responses of the API calls given
            valid requests, or malformed requests.

            The other, was system integration tests. These were more complex
            tests, as they were going to be verifying the integration between
            Identity(keystone), Compute, Load balancers and Autoscale. For
            example, When a user created a scaling group, it will verify that
            the minimum number of servers on the group are provisioned
            successfully, as in the servers are active and they exist as nodes
            on the load balancers. Or if a server, went into an error state,
            that can happen, Autoscale was able to re-provision that server.
          </aside>
        </section>

        <section>
          <h1>Testing Problems</h1>
          <h2>(with Auto Scale)</h2>
          <aside class="notes">
            All these tests were running against the real services. Servers
            could take over a minute, ten minutes to provision and the tests
            would run that much longer. Sometimes, the tests would fail due to
            random failures, like a server would go into error state, where as
            the test was expecting it to go into an active state. The tests for
            the negative scenarios, like actually testing how autoscale would
            behave if the server went into an error state, could not be
            tested. This is just not something that could be reproduced.
          </aside>
        </section>

        <section>
          <h1>More Problems</h1>
          <h2>(with Auto Scale)</h2>
          <aside class="notes">
            Well, the test coverage was improving as I continued to add tests,
            oblivious of the time it was taking run the entire test suite! Now,
            we had started using these tests as a gate in out merge
            pipeline. But the tests were running for so long and were sometimes
            flaky. Nobody dared to run these tests locally! Not even me, when I
            was adding more tests! Also, our peers from the compute and load
            balancers teams, whose resources we were using up for our
            "Auto-scale" testing, were _not_ happy! So much, so that, we were
            pretty glad, we were in a remote office!
          </aside>
        </section>

        <section>
          <h1>We've Had Enough!</h1>
          <h2>(on Auto Scale)</h2>
          <aside class="notes">
            This had to change! we needed something! to save us from the slow and flaky
            testing!
          </aside>
        </section>

        <section data-state="first-demo">
          <video src="bootstrap.mp4" id="first-demo-video"></video>
          <aside class="notes">
            As you can see here, it is only a 3 step process to get mimic
            started, and be able to hit the APIs it implements.  The steps are
            pip install mimic, run mimic and hit the endpoint! That's it.  As
            we mentioned, the way you hit the endpoint is to simply change your
            client's Identity URL to Mimic's and run your code as before.
          </aside>
        </section>


        <!--

            <section>
              <aside class="notes">
              </aside>
            </section>

         -->
      </div>

    </div>

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.min.js"></script>

    <script src="configuration.js"> </script>
    <script src="custom.js"></script>

  </body>
</html>

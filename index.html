<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">

    <title>Mimic Presentation</title>

    <meta name="description" content="A framework for easily creating beautiful presentations using HTML">
    <meta name="author" content="Hakim El Hattab">

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="custom.css">

    <link rel="stylesheet" href="css/reveal.min.css">
    <link rel="stylesheet" href="css/theme/default.css" id="theme">

    <!-- For syntax highlighting -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- If the query includes 'print-pdf', include the PDF print sheet -->
    <script>
      if( window.location.search.match( /print-pdf/gi ) ) {
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = 'css/print/pdf.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
      }
    </script>

    <!--[if lt IE 9]>
	<script src="lib/js/html5shiv.js"></script>
	<![endif]-->
  </head>

  <body>

    <div class="reveal">

      <!-- Any section element inside of this container is displayed as a slide -->
      <div class="slides">
        <section data-background="photo.png">
          <h1>Mimic</h1>
          <h2>An API Compatible Mock Service For OpenStack</h2>
        </section>

        <section>
          <h1>Lekha</h1>
          <h3 style="color: gray"> software developer in test</h3>
          <h3 style="color: gray"> (at) Rackspace </h3>
          <aside class="notes">
            Lekha: Hi, I am Lekha Jeevan, a software developer in test at Rackspace
          </aside>
        </section>

        <section>
          <h1>Glyph</h1>
          <h3 style="color: gray"> software developer </h3>
          <h3 style="color: gray"> (at) Rackspace </h3>
          <aside class="notes">
            Glyph: and I'm Glyph, also a software developer at Rackspace
          </aside>
        </section>

        <section>
          <h1>What?</h1>
          <aside class="notes">
            Lekha: Today Glyph and I are here to talk about Mimic. An
            opensource framework that allows for testing of OpenStack and
            Rackspace APIs.
            <br>
            Mimic has been making testing across products in Rackspace a cake
            walk. And we think it could do the same for the OpenStack- backed
            applications someday. Today, We are not there yet. But your
            contributions will help us get there soon.
          </aside>
        </section>

        <section>
          <h1>Sneak Peek</h1>
          <aside class="notes">

           Lekha: Before we even begin to dive into the details, let us take a
           quick sneak peek on how <b>easy</b> it is to get started with Mimic.

          </aside>
        </section>

        <section data-state="first-demo">
          <video src="bootstrap.mp4" id="first-demo-video"></video>
          <aside class="notes">
            It is <em>only</em> a 3 step process!
            In a virtual env, we pip install mimic, run mimic and hit the
            endpoint!
            <br> 
            And there is an endpoint to be able to Authenticate and get a
            service catalog containing the (OpenStack) services that Mimic
            implements.

          </aside>
        </section>

        <section>
          <h1>Why?</h1>
          <aside class="notes">

            Glyph: While Mimic is hopefully general purpose, it was originally
            created for testing a specific application: Rackspace Auto Scale.
            So let's talk about that.

          </aside>
        </section>

        <section>
          <h1>Auto Scale</h1>
          <aside class="notes">

            Lekha: Rackspace Auto Scale is a system which automates the process
            of getting the right amount of compute capacity for an application,
            by creating (scaling up) and deleting (scaling down) servers and
            associating them with load balancers.
            <!-- TO DO: A diagram indicating the above-->

          </aside>
        </section>

        <section>
          <h1>Dependencies</h1>
          <h2>(of Auto Scale)</h2>
          <aside class="notes">
            Lekha: In order to perform these tasks, Auto Scale speaks to three
            back-end Rackspace APIs.
          </aside>
        </section>

        <section>
          <h1>Rackspace Identity</h1>
          <aside class="notes">

            Lekha: Identity for authentication and impersonation.

          </aside>
        </section>

        <section>
          <h1>Rackspace Cloud Servers</h1>
          <aside class="notes">

            Lekha: Cloud Servers for provisioning and deleting servers.

          </aside>
        </section>

        <section>
          <h1>Rackspace Cloud Load Balancers</h1>
          <aside class="notes">

            Lekha: Rackspace Cloud Load Balancers for adding and removing servers to
            load balancers as they are created or deleted.

          </aside>
        </section>

        <section>
          <h2>Rackspace Identity</h2>
          <h3 style="color:orange"> is API compatible with </h3>
          <h2> Openstack Identity v2 </h2>
          <aside class="notes">

            Lekha: Rackspace Identity is API-compatible with OpenStack
            Identity v2.

          </aside>
        </section>

        <section>
          <h2>Rackspace Cloud Servers</h2>
          <h3 style="color:orange"> is API compatible with </h3>
          <h2> Openstack Compute </h2>
          <aside class="notes">

            Lekha: Rackspace Cloud Servers is powered by and API- compatible
            with OpenStack Compute.

          </aside>
        </section>

        <section>
          <h2>Rackspace Cloud Load Balancers</h2>
          <h3 style="color:orange"> is a </h3>
          <h2> Custom API </h2>
          <aside class="notes">
          <aside class="notes">

            Lekha: Although Rackspace Cloud Load Balancers is a custom API.

          </aside>
        </section>

        <section>
          <h1>Testing</h1>
          <h2 style="color: gray">(for Auto Scale)</h2>
          <aside class="notes">

            Lekha: As Auto Scale was interacting with so many other systems,
            testing Auto Scale did not mean <b>just</b> testing features of Auto
            Scale. But also that, if any of these systems it depended on did not
            behave as expected, Auto Scale did not crumble or crash, but was consistent
            and able to handle these upstream failures gracefully.

            <!-- TO DO: Include diagram to depict this. Something like-->
            <!-- Auto Scale interacting with systems and them not responding.-->
            <!-- But Auto Scale reponding to the end user-->

          </aside>
        </section>

        <section>
          <h1>Types of Tests</h1>
          <h2 style="color: gray">(for Auto Scale)</h2>
          <aside class="notes">
            Lekha: So, there were two kinds tests for Auto Scale:
          </aside>
        </section>

        <section>
          <h1>Functional</h1>
          <h2>(API contracts)</h2>
          <aside class="notes">

            Lekha: One, was the Functional tests to validate the API
            contracts. These tests verified the responses of the Auto Scale
            API calls given valid, or malformed requests.

          </aside>
        </section>

        <section>
          <h1>Types of Tests</h1>
          <h2>System Integration</h2>
          <br>
          <br>

          <center>
            <table>
              <tr>
                <td>
                  <h2>Auto Scale</h2>
                </td>
                <td>
                  <h2>→</h2>
                </td>
                <td>
                  <h2>Identity</h2>
                </td>
              </tr>
              <tr>
                <td>
                </td>
                <td>
                  <h2>→</h2>
                </td>
                <td>
                  <h2>Compute</h2>
                </td>
              </tr>
              <tr>
                <td>
                </td>
                <td>
                  <h2>→</h2>
                </td>
                <td>
                  <h2>Load Balancers</h2>
                </td>
              </tr>
            </table>
          </center>

          <aside class="notes">

            Lekha: And the other was the System integration tests. Theese,
            were more complex. These tests verified integration between Auto
            Scale and Identity, Compute, and Load balancers.

          </aside>
        </section>

        <section>
          <h1>Types of Tests</h1>
          <h2>System Integration</h2>
          <h2>(Example 1)</h2>

          <aside class="notes">

            Lekha: For example: When a scaling group was created, one such test will verify
            that the servers on that group were provisioned successfully. That
            is, that the servers went into an 'active' state and were added as a nodes to the
            load balancer on the sceling group.

                        <!-- Include diagram to depict this and use a better heading ? -->
          </aside>
        </section>

        <section>
          <h1>Types of Tests</h1>
          <h2>System Integration</h2>
          <h2>(Example 2)</h2>

          <aside class="notes">

            Lekha: Or, if a server went into an error state, (yes! that can happen!),
            Auto Scale was able to re-provision that server succefully, and then add that
            active server to the load balancer on the scaling group.

            <!-- Include diagram to depict this and use a better heading ? -->


          </aside>
        </section>

        <section>
          <h1>Testing Problems</h1>
          <h2>(with Auto Scale)</h2>
          <aside class="notes">

            Lekha: All these tests were set up to run against the real services.
            And... here are some observations I had while writing the tests:
            Servers could take over a minute, or ten minutes, or longer to
            provision. And the tests would run that-much-longer.
            <br>
            Sometimes, the tests would fail! due to raandom upstream failures.
            Like a server would go into 'error' state, where as the <b>test</b> was
            expecting it to go into an 'active' state.
            <br>
            And tests for such negative scenarios, like actually testing how
            Auto Scale would behave if the server did go into 'error' state,
            could not be tested. This is something that could <b>not</b> be
            reproduced consistently.

          </aside>
        </section>

        <section>
          <h1>More Problems</h1>
          <h2>(with Auto Scale)</h2>
          <aside class="notes">

            Lekha: Well, the overall test coverage was improving. And I
            continued to add tests, oblivious of the time it was taking run
            the entire test suite!
            <br>
            Later, we had started using these tests as a <em>gate</em>, in our merge
            pipeline.
            <br>
            But, the tests were running for <b>so</b> long and were sometimes flaky.
            Nobody dared to run these tests locally! Not even me, when I was
            adding more tests!
            <br>
            Also, our peers from the compute and load balancers teams, whose
            resources we were using up for our "Auto-scale" testing, were
            <em>not</em> happy! So much, so that, we were <em>pretty</em> glad, we were in a
            remote office!

            <!--TO DO: Pictures to depict each para above-->

          </aside>
        </section>

        <section>
          <h1>We've Had Enough!</h1>
          <h2>(on Auto Scale)</h2>
          <aside class="notes">

            Lekha: But! We had had enough! This had to change! we needed
            something! to save us from these slow flaky tests!

          </aside>
        </section>

        <section>
          <h1>There And Back Again</h1>
          <h2 class="fragment current-visible">Specific → General</h2>
          <h2 class="fragment current-visible">General → Specific</h2>
          <aside class="notes">
            Glyph: Now that we've had enough, how are we going to solve this
            problem?  Since we've been proceeding from the specific case of
            Auto Scale to the general utility of Mimic, (click) let's go back to
            the general and proceed to the specific.
          </aside>
        </section>

        <section>
          <h1>General →</h1>
          <h2 class="fragment visible">Testing For Failure</h2>

          <aside class="notes">
            Glyph: When you have a complex distributed system - that is, a
            service A which makes requests of another service B - you have to
            deal with the whole spectrum of real failures which might come back
            from service B.
          </aside>
        </section>
        <section>
          <h1>Real Services Fail</h1>
          <aside class="notes">
            Glyph: Testing against the real service is of course important for
            validating a product, ensuring that it works as expected in a
            realistic deployment environment, even if your colleagues on the
            compute and load balancer teams get mad at you.
          </aside>
        </section>

        <section>
          <h1>(Unpredictably)</h1>
          <aside class="notes">
            Glyph: But when you write code to interact with a service, you need
            to handle a wide variety of error cases.  Those errors won't come
            back from the real service every time.
          </aside>
        </section>

        <section>
          <h1>Succeeding</h1>
          <h2>At Success</h2>
          <aside class="notes">
            Glyph: Your positive-path code - the code that submits a request
            and gets the response that it expects - is going to get lots of
            testing in the real world.  Most interactions with services are
            successful, and the operators of those services always strive to
            make ever more of those interactions successful.  So most likely,
            the positive-path code is going to get exercised all the time and
            you will have plenty of opportunities to flush out bugs.
          </aside>
        </section>
        <section>
          <h1>Means Failing</h1>
          <h2>At Failure</h2>
          <aside class="notes">
            Glyph: If you test against real services, your negative-path code,
            will only get invoked in production when there's a real error.  If
            everything is going as planned, this should be infrequent.
          </aside>
        </section>
        <section>
          <h1>Mimic Succeeds</h1>
          <h2>At Failure!</h2>
          <aside class="notes">
            Glyph: It's really important to get negative-path code right.  If
            everything is going well, it's probably okay if your code has a
            couple of bugs.  You might be able to manually work around them.
          </aside>
        </section>
        <section>
          <h1 style="font-size: 800%">😈 ☁</h1>
          <h3>(Production)</h3>
          <aside class="notes">
            Glyph: But if things are starting to fail with some regularity in
            your cloud - that is to say - if you are using a cloud - that is
            exactly the time you want to make sure <em>your</em> system is
            behaving correctly: accurately reporting the errors, measuring the
            statistics, and allowing you to stay on top of incident management
            for your service.
          </aside>
        </section>
        <section>
          <h1 style="font-size: 800%">😇 ☁</h1>
          <h3>(Staging)</h3>
          <aside class="notes">
            Glyph: When you test against a real service, you are probably
            testing against a staging instance.  And, if your staging instance
            is typical, it probably doesn't have as much hardware, or as many
            concurrent users, as your production environment.  Every additional
            piece of harware or concurrent user is another opportunity for
            failure, so that means your staging environment is probably even
            less likely to fail.
          </aside>
        </section>
        <section>
          <pre style="border: 1px solid white; font-size: 70px;"><code class="language-python">import unittest</code></pre>
          <aside class="notes">
            Glyph: I've been in the software industry for long enough now to
            remember where this part of the talk would be the hardest part -
            the part where we try to sell the idea that code coverage and
            automated testing is really important.  Luckily we have moved on
            from the bad old days of the 90s, when most teams didn't have build
            automation, and if they wanted it they might not even be able to
            afford it.
          </aside>
        </section>
        <section>
          <h2 style="text-transform: none;"><code>test_stuff ... <span style="color: rgb(0, 255, 0);">[OK]</span></code></h2>
          <aside class="notes">
            Glyph: Luckily today we are all somewhat more enlightened, and we
            know that testing is important and full code coverage is important.
            So when we write code like this:
          </aside>
        </section>
        <section>
          <pre style="font-size: 40px"><code class="language-python" style="padding: 1em;">try:
    result = service_request()
except:
    return error
else:
    return ok(result)</code></pre>
          <aside class="notes">
            Glyph: ... we know that we need to write tests for this part:
          </aside>
        </section>
        <section>
<pre style="font-size: 40px"><span class="codelike" style="padding: 1em;"><span class="keyword">try</span>:
    result = service_request()
<div class="highlighted"><span class="keyword">except</span>:
    <span class="keyword">return</span> error
</div><span class="keyword">else</span>:
    <span class="keyword">return</span> ok(result)</span></pre>
          <aside class="notes">
            Glyph: ... and one popular way to get test coverage for those error
            lines is by writing a custom mock for it.
          </aside>
        </section>
        <section>
          <img src="Alice_par_John_Tenniel_34_Cropped.png">
          <aside class="notes">
            Glyph: The problem with the traditional definition of a mock is that each
            mock is defined just for the specific test that it's mocking.
          </aside>
        </section>
        <section>
          <img src="Alice_par_John_Tenniel_34_Cropped_Tears.png">
          <aside class="notes">
            Glyph: The problem with the traditional definition of a mock is
            that each mock is defined just for the specific test that it's
            mocking.  There's a reason that the Mock Turtle is crying.  Mocking
            is often a sad enterprise.
            <p>
              Let's take a specific example from OpenStack Compute.
            </p>
          </aside>
        </section>
        <section>
          <pre style="font-size: 24pt;"><code class="language-python">if not os.chdir(ca_folder(project_id)):
    raise exception.ProjectNotFound(
        project_id=project_id)</code></pre>
          <aside class="notes">
            Glyph: In June of this year, OpenStack
              Compute <a href="https://github.com/openstack/nova/commit/1a6d32b9690b4bff709dc83bcf4c2d3a65fd7c3e">introduced
              a bug</a> making it impossible to revoke a certificate.  The
              lines of code at fault were these two additions here.

            This is not a criticism of Nova itself;
              [<a href="https://github.com/openstack/nova/commit/96b39341d5a6ea91d825d979e2381b9949b26e27">the
              bug was later fixed</a>.
          </aside>
        </section>
        <section>
          <pre style="font-size: 24pt;"><span class="codelike language-python"><span class="highlighted"><span class="keyword">if</span> <span class="keyword">not</span> os.chdir</span>(ca_folder(project_id)):
    <span class="keyword">raise</span> exception.ProjectNotFound(
        project_id=project_id)</span></pre>
          <aside class="notes">
          </aside>
        </section>
        <section>
          <pre style="font-size: 17pt"><code class="language-python">@mock.patch.object(os, 'chdir', return_value=True)
def test_revoke_cert_process_execution_error(self):
    "..."

@mock.patch.object(os, 'chdir', return_value=False)
def test_revoke_cert_project_not_found_chdir_fails(self):
    "..."</code></pre>
          <aside class="notes">
            Glyph: The bug here is that `chdir` does not actually return a
            value.  Because these tests construct their own mocks for `chdir`,
            we properly cover all the code, but the code is not integrated with
            a system that is verified in any way against what the real system
            (in this case, Python's chdir) does.
          </aside>
        </section>
        <section>
          <img src="folders.png" />
          <aside class="notes">
            Glyph: In this <em>specific</em> case, we might have simply tested
            against a real directory structure in the file system, because
            relative to the value of testing against a real implementation,
            creating a directory is not a terribly expensive operation.
          </aside>
        </section>
        <section>
          <img src="openstack_havana_conceptual_arch.png" />
          <aside class="notes">
            Glyph: However, an OpenStack cloud is a significantly more complex
            system than `chdir`.  If you are developing an application that
            depends on OpenStack, creating a real cloud to test against is far
            too expensive and slow as Auto Scale's experience shows.  Creating
            a one-off mock for every test is fast to get started with and fast
            to run, but is error prone and rapidily becomes a significant
            maintainance burden of its own right. Auto Scale needed something
            that was quick to deploy, and lightweight to run like a mock, but
            realistic and usable in an integration scenario like a __real__
            system.
          </aside>
        </section>
        <section>
          <h1>→Specific</h1>
          <h2 class="fragment visible">Mimic</h2>
          <aside class="notes">
            Glyph: Since we've been proceeding from the general to the
            specific, right here, where we need a realistic mock of a back-end
            openstack service, is where the specific value of Mimic comes in.
          </aside>
        </section>
        <section>
          <h1>Mimic</h1>
          <h2>Version 0.0</h2>
          <aside class="notes">
            Lekha: The first version of Mimic was built as a stand-in service
            for Identity, Compute and Rackspace Load balancers, the services
            that Auto Scale depends on.
          </aside>
        </section>
        <section>
          <h1>Pretending</h1>
          <h2 style="visibility: hidden">...</h2>
          <aside class="notes">
            Lekha: The essence of Mimic is pretending. The first thing that you
            must do to interact with it is to...
          </aside>
        </section>
        <section>
          <h1>Pretending</h1>
          <h2>to authenticate</h2>
          <aside class="notes">
            Lekha: ...pretend to authenticate.
            <br>

            Mimic does not validate credentials - all authentications will
            succeed. As with the real Identity endpoint, Mimic's identity
            endpoint has a service catalog which includes URLs for all the
            services implemented within Mimic. <br>

            A well behaved OpenStack client will use the service catalog to
            look up URLs for its service endpoints. Such a client will only
            need two pieces of configuration to begin communicating with the
            cloud, i.e. credentials and the identity endpoint. A client
            written this way will only need to change the Identity endpoint to
            be that of Mimic.

          </aside>
        </section>
        <section>
          <h1>Pretending</h1>
          <h2>to Boot Servers</h2>
          <aside class="notes">
            Lekha: When you ask Mimic to create a server, it pretends to create
            one.  This is not like stubbing with static responses: when Mimic
            pretends to build a server, it remembers the information about that
            server and will tell you about it in the subsequent requests.
          </aside>
        </section>
        <section>
          <h1>Pretending</h1>
          <h2>is <em>faster</em> than working</h2>
          <aside class="notes">

            Lekha: Mimic was originally created to speed things up. So, it was
            very important that it be fast both to respond to requests and to
            have developers setup.

          </aside>
        </section>

        <section>
          <h1>in-memory data structures</h1>
          <aside class="notes">
            Lekha: It was built using in-memory data structures.
          </aside>
        </section>

        <section>
          <h1>minimal software dependencies</h1>
          <p text-transform:lowercase> (almost entirely pure Python) </p>
          <aside class="notes">
            Lekha: with minimal software dependencies, almost entirely pure Python.
          </aside>
        </section>

        <section>
          <h1>
            <span style='color:black;text-decoration:line-through'>
              <span style='color:white'>Service Dependencies</span>
            </span>
          </h1>
          <aside class="notes">
            Lekha: No service dependencies
          </aside>
        </section>

        <section>
          <h1>
          <span style='color:black;text-decoration:line-through'>
              <span style='color:white'>Configuration</span>
            </span>
          </h1>
          <aside class="notes">
            Lekha: No configuration
          </aside>
        </section>

        <section>
          <h1>self-contained</h1>
          <aside class="notes">
            Lekha: And entirely self-contained.
          </aside>
        </section>

        <section>
          <h1>Demo!</h1>
          <aside class="notes">
            Lekha: Here is a quick demo.
          </aside>
        </section>
        <section>
          <video src="python-client-demo.mp4">
          <aside class="notes">
            Lekha: Here is an example of how we could use Mimic against the
            OpenStack nova python client.  We're including the Mimic identity
            endpoint as the `AUTH_URL`.
          </aside>
        </section>

        <section>
          <h1>Using Mimic</h1>
          <h2>(on Auto Scale)</h2>
          <aside class="notes">

            Lekha: We did the same thing with Auto Scale. We pointed the tests
            and Autoscale to run against an instance of Mimic.

          </aside>
        </section>


        <section>
          <h1>The Results!</h1>
          <h2>(Functional tests using Mimic)</h2>
          <aside class="notes">

            Lekha: This reduced the test time exponentially! Before Mimic the
            functional tests would take...

          </aside>
        </section>

        <section>
          <h3>Functional Tests:</h3>
          <h1>15 minutes</h1>
          <h3>against a real system</h3>
          <h2>vs.</h2>
          <h1>30 seconds</h1>
          <h3>against Mimic</h3>
          <aside class="notes">

            Lekha: 15 minutes to complete, and now they run in less than 30
            seconds!

          </aside>
        </section>

        <section>
          <h1>The Results!</h1>
          <h2>(System Integration tests using Mimic)</h2>
          <aside class="notes">
            Lekha: The system integration tests would take 3 *hours* or more,
            cause if one of the servers in the test remained in the "building"
            state for fifteen minutes longer than usual, then the tests ran
            fifteen minutes slower.
          </aside>
        </section>

        <section>
          <h3>System Integration Tests:</h3>
          <h1>3 hours or more</h1>
          <h3>against a real system</h3>
          <h2>vs.</h2>
          <h1>3 minutes</h1>
          <h3>against Mimic</h3>
          <aside class="notes">

            Lekha: This went down to be less than 3 *minutes* (consistently) to
            complete!

          </aside>
        </section>

        <section>
          <h1 style="font-size: 800px;">✈</h1>
          <aside class="notes">

            Lekha: Our dev VMs are now configured to run against Mimic.

            One of our devs from the Rackspace Cloud Intelligence team, calls
            this "Developing on Airplane Mode!", as devs get immediate
            feedback on the code being written and we can work offline without
            having to worry about uptimes of the upstream systems.

          </aside>
        </section>
        <section>
          <h1> What about<br>error injection? </h1>
          <aside class="notes">
            Glyph: But Lekha, what about all the error injection stuff I
            mentioned before?  Does Mimic do that?  How did this set-up test
            Auto Scale's error conditions?
          </aside>
        </section>

        <section>
          <h1> Mimic does<br>simulate errors </h1>
          <aside class="notes">

            Lekha: Well Glyph, I am as pleased as I am suprised that you ask
            that.  Mimic does simulate errors.

          </aside>
        </section>

        <section>
          <h1>Error injection using <code>metadata</code></h1>
          <aside class="notes">

            Lekha: Earlier, when I said Mimic pretends to create a server, that
            wasn't entirely true - sometimes Mimic pretends to *not* create a
            server.  It uses the metadata provided during the creation of the
            server, inspects the metadata, and sets the state of the server
            respectively.

          </aside>
        </section>

        <section>
          <video src="error-injection-demo.mp4"></video>
          <aside class="notes">

            Lekha: Lets go back to the demo and see how this can be done. We
            are creating a server with `metadata`: `"server_building": 30`.
            This... will make the server stay in building state for 30
            seconds. Similarly, servers can also be made to go into error state
            on creation, using the `metadata`: `"server_error": True`.

          </aside>
        </section>

        <section>
          <h1>Retry On Errors</h1>
          <aside class="notes">

            Lekha: For the purposes of Auto Scale it was important that we have
            right number of servers on a scaling group, even if a number of
            attempts to create one failed. We chose to use `metadata` for error
            injection so that requests with injected errors could also be run
            against real services. For Auto Scale, the expected end result is
            the same number of servers created, irrespective of the number of
            failures. But this behavior may also be useful to many other
            applications because retrying is a common pattern for handling
            errors.

          </aside>
        </section>

        <section>
          <h2>Mimic 0.0 was...</h2>
          <h1>Too Limited</h1>
          <aside class="notes">

            Glyph: However, the first implementation of mimic had some flaws,
            it was fairly Rackspace specific and only implemented the endpoints
            of the services that Autoscale depends upon . And they were all
            implemented as part of Mimic's core. It ran each service on a
            different port, meaning that for N endpoints you would need not
            just N port numbers, but N *consecutive* port numbers. It allowed
            for testing error scenarios, but only using the metadata. This was
            not useful for all cases, for example, for a control panel that
            does not not allow every UI action to contain
            metadata.  <!-- (TODO: ask someone if Horizon matches this
            description?) -->

          </aside>
        </section>
        <section>
          <h2>Mimic 0.0 was...</h2>
          <h1>Single Region</h1>
          <aside class="notes">
            Glyph: Mimic also did not implement multiple regions.  It used
            global variables for storing all state, which meant that it was
            hard to add additional endpoints with different state in the same
            running mimic instance.
          </aside>
        </section>
        <section>
          <h2>Beyond Auto Scale:</h2>
          <h1>Refactoring Mimic</h1>
          <aside class="notes">
            Glyph: Mimic had an ambitious vision: to be a one-stop mock for all
            OpenStack and Rackspace services that needed fast integration
            testing.  However, its architecture at the time severely limited
            the ability of other teams to use it or contribute to it.  As Lekha
            mentioned, it was specific not only to Rackspace but to Auto Scale.
          </aside>
        </section>
        <section>
          <h1>YAGNI</h1>
          <aside class="notes">
            Glyph: On balance, Mimic was also extremely simple.  It followed
            the You Aren't Gonna Need It principle of extreme programming very
            well, and implemented just the bare minimum to satisfy its
            requirements, so there wasn't a whole lot of terrible code to throw
            out or much unnecessary complexity to eliminate.
          </aside>
        </section>
        <section>
          <h1>E(ITO)YAGNI</h1>
          <aside class="notes">
            Glyph: There is, however, a corrolary to YAGNI, which is
            E(ITO)YAGNI: Eventually, It Turns Out, You *Are* Going To Need It.
            As Mimic grew, other services within Rackspace wanted to make use
            of its functionality, and a couple of JSON response dictionaries in
            global variables were not going to cut it any more.
          </aside>
        </section>
        <section>
          <h1>Plugins!</h1>
          <aside class="notes">
            Glyph: So we created a plugin architecture.
          </aside>
        </section>
        <section>
          <h1>Identity</h1>
          <h2>Is the Entry Point</h2>
          <h3>(Not A Plugin)</h3>
          <aside class="notes">
            Glyph: As Lekha mentioned previously, Mimic's Identity endpoint was
            the top-level entry point to Mimic as a service; every other URL
            was available from within the service catalog.  As we were
            designing the plugin API, it was clear that this top-level Identity
            endpoint needed to be the core part of Mimic, and plug-ins would
            each add an entry for themselves to the service catalog.
          </aside>
        </section>
        <section>
          <h1><code>IAPIMock</code></h1>
          <h2>Plugin Interface</h2>

          <aside class="notes">
            Glyph: Each plugin implements the `IAPIMock` interface, which has
            only two methods:
          </aside>
        </section>
        <section class="apimock">
          <pre><code class="language-python">class IAPIMock():</code></pre>
          <pre class="fragment"><code class="language-python">  def catalog_entries(...)</code></pre>
          <pre class="fragment"><code class="language-python">  def resource_for_region(...)</code></pre>
          <br />
          <h3 class="fragment">(that's it!)</h3>

          <aside class="notes">
            Glyph: (click) `catalog_entries` (click) and `resource_for_region`.
          </aside>
        </section>

        <section class="apimock">
          <pre><code class="language-python"
>def catalog_entries(self,
                    tenant_id):</code></pre>
          <aside class="notes">
            <p>
              Glyph: `catalog_entries` takes a tenant ID and returns the
              entries in Mimic's service catalog for that particular API mock.

            <p>
              APIs have catalog entries for each API type, which in turn have
              endpoints for each virtual region they represent.

            <p>
              This takes the form of an iterable of a class called `Entry`,
              each of which is a name and a collection of `Endpoint` objects,
              each containing a region, a URI version prefix, and a tenant ID
              of its own.
          </aside>
        </section>
        <section class="apimock">
          <pre><code class="language-python"
>def resource_for_region(
    self, region, uri_prefix,
    session_store
):</code></pre>
          <aside class="notes">
            Glyph: `resource_for_region` takes the name of a region, a URI
            prefix - produced by Mimic core to make URI for each service unique
            - and a session store where the API mock may look up state of the
            resources it pretended to provision for the respective
            tenants. `resource_for_region` returns an HTTP resource associated
            with the top level of the given region.  This resource then routes
            this request to any tenant- specific resources associated with the
            full URL path.
          </aside>
        </section>
        <section>
          <pre style="font-size: 30pt;"><code class="language-python"
># mimic/plugins/your_plugin.py

from your_api import SomeAPIMock
the_mock_plugin = SomeAPIMock()
</code></pre>
          <aside class="notes">
            Glyph: Mimic uses the Twisted plugin system, which has a very
            simple model of how plugins work: you have an abstract interface -
            some methods and attributes that you expect of a plugin - and then
            plugins register themselves by instantiating a provider of that
            interface and placing that instance into a module within a
            particular namespace package.  In Mimic's case, that interface is
            `IAPIMock` and that package is `mimic.plugins`.
          </aside>
        </section>
        <section>
          <aside class="notes">
            Glyph: Each service can contribute to the service catalog, provide
            entries and endpoints, and each endpoint gets added within the URI
            hierarchy.

            <!-- TODO: Demo of implementing a plugin! -->

          </aside>
        </section>
        <!-- TODO: insert a segue or transition here! -->
        <section>
          <h1>Error Conditions Repository</h1>
          <aside class="notes">
            Lekha: Anyone testing a product, will run into unexpected
            errors. Thats why we test!  But we dont know what we dont know, and
            cant be prepared for this ahead of time right!
          </aside>
        </section>
        <section>
          <h1>Discovering Errors</h1>
          <h2>Against Real Services</h2>
          <aside class="notes">
            Lekha: When we were running the Auto Scale tests against Compute,
            we began to see some one-off errors. Like, when provisioning a
            server, the test expected a server to go into a building state for
            some time before it is active, __but__ it would remain in building
            state for over an hour or even would sometimes go into an error
            state.
          </aside>
        </section>
        <section>
          <h1>Record Those Errors</h1>
          <h2>Within Mimic</h2>
          <aside class="notes">
            Lekha: Auto Scale had to handle such scenarios gracefully and the
            code was changed to do so. And Mimic provided for a way to tests
            this consistently.
          </aside>
        </section>
        <section>
          <h1>Discover More Errors</h1>
          <h2>Against Real Services</h2>
          <aside class="notes">
            Lekha: However, like I said, we dont know what we dont know. We
            were not anticipating any other such errors. But, there were more!
            And this was slow process for us to uncover such errors as we
            tested against the real systems. And we continued to add them to
            Mimic.
          </aside>
        </section>
        <section>
          <h1>Record Those Errors</h1>
          <h2>For The Next Project</h2>
          <aside class="notes">
            Lekha: Now, Wont it be great if not every person writing an
            application that used compute as a dependency had to go through
            this same cycle and have to find all the possible error conditions
            in a system by experience and have to deal with them at the pace
            that they occur.
          </aside>
        </section>
        <section>
          <h1>Share A Repository</h1>
          <h2>For Future Projects</h2>
          <aside class="notes">
            Lekha: What if we had a repository of all such known errors, that
            everyone contributes to. So the next person using the plugin can
            use the existing ones, and ensure there application behaves
            consistently irrespective of the errors, and be able add any new
            ones to it.
          </aside>
        </section>
        <section>
          <h1>Mimic Is A Repository</h1>
          <aside class="notes">
            Lekha: Mimic is just that, a repository of all known responses
            including the error responses.
          </aside>
        </section>

        <section>
          <h1>Control</h1>
          <aside class="notes">
            Glyph: In addition to storing a repository of errors, Mimic allows
            for finer control of behavior beyond simple success and error.  You
            can determine the behavior of a mimicked service in some detail.
          </aside>
        </section>

        <section>
          <h1>Now &amp; Later</h1>
          <aside class="notes">
            Glyph: We're not just here today to talk about exactly what Mimic
            offers right now, but where we'd like it to go.  And in that spirit
            I will discuss one feature that Mimic has for controlling behavior
            today, and one which we would like to have in the future.
          </aside>
        </section>

        <section>
          <h1>Now</h1>
          <aside class="notes">
            Glyph: Appropriately enough, since I'm talking about things now and
            things in the future, the behavior-control feature I'd like to talk
            about that that Mimic has right now is the ability to control time.
          </aside>
        </section>
        <section>
          <h1><code>now()</code></h1>
          <aside class="notes">
            Glyph: That is to say: when you do something against Mimic that
            will take some time, such as building a server, time does not
            actually pass ... for the purposes of that operation.
          </aside>
        </section>
        <section>
          <h2><code>/mimic/v1.1/tick</code></h2>
          <aside class="notes">
            Glyph: Instead of simply waiting 10 seconds, you can hit this
            second out-of-band endpoint, the "tick" endpoint ...
          </aside>
        </section>
        <section class="apimock">
          <pre><code class="language-javascript"
>{
    "amount": 100.0
}
</code></pre>
          <aside class="notes">
            Glyph: with a payload like this.  It will tell you that time has
            passed, like so:
          </aside>
        </section>
        <section>
          <pre style="font-size: 34px"><code class="language-javascript"
>{
    "advanced": 1.0,
    "now": "1970-01-01T00:00:02.000000Z"
}</code></pre>
          <aside class="notes">
            Glyph: with a payload like this.  It will tell you that time has
            passed, like so.  Now, you may notice there's something a little
            funny about that timestamp - it's suspiciously close to midnight,
            january first, 1970.  Mimic begins each subsequent restart thinking
            it's 1970, at the unix epoch; if you want to advance the clock,
            just plug in the number of seconds since the epoch as the "amount"
            and your mimic will appear to catch up to real time.
          </aside>
        </section>
        <section>
          <pre style="font-size: 34px"><code class="language-javascript"
>{
    "server": {
        "status": "BUILD",
        "updated": "1970-01-01T00:00:00.000000Z",
        "OS-EXT-STS:task_state": null,
        "user_id": "170454",
        "addresses": {
            "public": [
                {
                    "version": 4,
                    "addr": "198.101.241.239"
          </code></pre>
          <aside class="notes">
            Glyph: If you've previously created a server with "server_building"
            metadata that tells it to build for some number of seconds, and you
            hit the 'tick' endpoint telling it to advance time the
            server_building number of seconds...
          </aside>
        </section>
        <section>
          <pre style="font-size: 34px"><code class="language-javascript"
>{
    "server": {
        "status": "ACTIVE",
        "updated": "1970-01-01T00:00:00.000000Z",
        "OS-EXT-STS:task_state": null,
        "user_id": "170454",
        "addresses": {
            "public": [
                {
                    "version": 4,
                    "addr": "198.101.241.239"
          </code></pre>
          <aside class="notes">
            Glyph: that server (and any others) will now show up as "active",
            as it should.  This means you can set up very long timeouts, and
            have servers behave "realistically", but in a way where you can
            test several hours of timeouts a time.
          </aside>
        </section>

        <section>
          <h1>Later</h1>
          <aside class="notes">
            Glyph: Another feature that we hope to design later is the ability
            to inject errors ahead of time, using a separate control-plane
            interface which is not part of a mock's endpoint.
          </aside>
        </section>

        <section>
          <h1> Error Injection </h1>
          <aside class="notes">
            Glyph: We've already begun work on doing this for Compute, but we
            feel that every service should have the ability to inject arbitrary
            errors.
          </aside>
        </section>

        <section>
          <h1>Even Later...</h1>
          <aside class="notes">
            Glyph: But that's not the only feature that we'd like to implement
            one day.  What we'd really like is to start to build a community
            around these testing practices, cataloguing errors, and making it
            possible to experiment with cloud APIs in a very lightweight way.
            We need <em>your</em> features.
          </aside>
        </section>

        <section>
          <h1>Call to Action</h1>
          <aside class="notes">
            Lekha: Mimic, can be the tool, where you do not have to stand up
            the entire dev stack to understand how an OpenStack API behaves.

            <p>
              Mimic can be the tool which enables an OpenStack developer to get
              quick feedback on the code he/she is writing and not have to go
              through the gate multiple times to understand that, maybe I
              should have handled that one error, that the upstream system
              decides to throw my way every now and then.
          </aside>
        </section>

        <section>
          <h1>It's Easy!</h1>
          <aside class="notes">
            Glyph: One of the things that I like to point out is that Mimic is
            not real software.  It's tiny, self-contained, doesn't need to
            interact with a database, or any external services.  Since it
            mimics exclusively existing APIs, there are very few design
            decisions.  As a result, contributing to Mimic is a lot easier than
            contributing to OpenStack proper.
          </aside>
        </section>

        <!--

            <section>
              <aside class="notes">
              </aside>
            </section>

            -->

      </div>

    </div>

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.min.js"></script>

    <script src="configuration.js"> </script>
    <script src="custom.js"></script>

  </body>
</html>

<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">

    <title>Mimic Presentation</title>

    <meta name="description" content="A framework for easily creating beautiful presentations using HTML">
    <meta name="author" content="Hakim El Hattab">

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="custom.css">

    <link rel="stylesheet" href="css/reveal.css">
    <link rel="stylesheet" href="css/theme/default.css" id="theme">

    <!-- For syntax highlighting -->
    <link rel="stylesheet" href="lib/css/zenburn.css">

    <!-- If the query includes 'print-pdf', include the PDF print sheet -->
    <script>
      if( window.location.search.match( /print-pdf/gi ) ) {
      var link = document.createElement( 'link' );
      link.rel = 'stylesheet';
      link.type = 'text/css';
      link.href = 'css/print/pdf.css';
      document.getElementsByTagName( 'head' )[0].appendChild( link );
      }
    </script>

    <!--[if lt IE 9]>
	<script src="lib/js/html5shiv.js"></script>
	<![endif]-->
  </head>

  <body>

    <div class="reveal">

      <!-- Any section element inside of this container is displayed as a slide -->
      <div class="slides">
        <section data-background="photo.png">
          <h1>Mimic</h1>
          <h2>An API Compatible Mock Service For OpenStack</h2>
        </section>

        <section>
          <h1>Lekha</h1>
          <h3 class="deemphasized"> software developer in test</h3>
          <h3 class="deemphasized"> (at) Rackspace </h3>
          <!--img src="lekha.jpg" width=290 length=200-->
<center>
  <table>
    <tr> <td class="contactlabel">github:</td> <td class="contactinfo">lekhajee</td> </tr>
    <tr> <td class="contactlabel">freenode irc:</td> <td class="contactinfo">lekha</td> </tr>
    <tr> <td class="contactlabel">twitter:</td> <td class="contactinfo">@lekha_j1</td> </tr>
  </table>

</center>

<aside class="notes">
            Lekha: Hi, I am Lekha Jeevan, a software developer in test at Rackspace
          </aside>
        </section>

        <section>
          <h1>Glyph</h1>
          <h3 class="deemphasized"> software developer </h3>
          <h3 class="deemphasized"> (at) Rackspace </h3>
<center>
  <table>
    <tr> <td class="contactlabel">github:</td> <td class="contactinfo">glyph</td> </tr>
    <tr> <td class="contactlabel">freenode irc:</td> <td class="contactinfo">glyph</td> </tr>
    <tr> <td class="contactlabel">twitter:</td> <td class="contactinfo">@glyph</td> </tr>
  </table>

</center>

<aside class="notes">
            Glyph: and I'm Glyph, also a software developer at Rackspace
          </aside>
        </section>

        <section>
          <h1>What?</h1>
          <aside class="notes">
            Lekha: Today Glyph and I are here to talk about Mimic. An
            opensource framework that allows for testing of OpenStack and
            Rackspace APIs.
            <br>
            Mimic has been making testing across products in Rackspace a cake
            walk. And we think it could do the same for the OpenStack- backed
            applications someday. Today, We are not there yet. But your
            contributions will help us get there soon.
          </aside>
        </section>

        <section>
          <h1>Who?</h1>
          <aside class="notes">
            Lekha: I originally created Mimic to help with testing Rackspace
            Auto Scale, but as I was creating it I realized I wanted to make
            something re-usable that many projects across Rackspace and
            OpenStack could use.
            <br>
            Glyph: I came on to the project later on and I've been helping to
            flesh out its architecture, making it more generic and modular.
          </aside>
        </section>

        <section data-background="rs.jpg">
          <h1>Sneak Peek</h1>
          <aside class="notes">
           Lekha: Before we even begin to dive into the details, let us take a
           quick sneak peek to see how <b>easy</b> it is to get started with Mimic.

          </aside>
        </section>

        <section data-state="first-demo">
          <video src="bootstrap.mp4" id="first-demo-video"></video>
          <aside class="notes">
            Lekha: It is <em>only</em> a 3 step process!  In a virtual env, we 
            pip install mimic, run mimic and hit the endpoint!
            <br>
            And Mimic returns the Authentication endpoint, to be able to Authenticate, 
            and get a service catalog containing the (OpenStack) services that Mimic
            implements.
          </aside>
        </section>

        <section data-background="rewind.jpg">
          <h1>Why?</h1>
          <aside class="notes">
            Lekha: While Mimic is hopefully general purpose, it was originally
            created for testing a specific application: Rackspace Auto Scale.
            So let's talk about that.
          </aside>
        </section>

        <section>
          <h1>Rackspace Auto Scale</h1>
          <p style="color: orange">An opensource project</p>
          <a href="https://github.com/rackerlabs/otter">Source: https://github.com/rackerlabs/otter</a>
          <aside class="notes">
            Lekha: Rackspace Auto Scale is an open source project. A solution
            that Rackspace utilizes to automate the process of getting the right
            amount of compute capacity for an application, by creating (scaling up)
            and deleting(scaling down) servers and associating them with 
            load balancers.

          </aside>
        </section>

          <section>
            <h1>Dependencies</h1>
            <h2>(of Auto Scale)</h2>
            <aside class="notes">
              Lekha: In order to perform these tasks, Auto Scale speaks to three
              back-end Rackspace APIs.
            </aside>
          </section>

          <section>
            <h1>Rackspace<br>Identity</h1>
            <aside class="notes">
              Lekha: Identity for authentication and impersonation.

            </aside>
          </section>

          <section>
            <h1>Rackspace<br>Cloud Servers</h1>
            <aside class="notes">
              Lekha: Cloud Servers for provisioning and deleting servers.

            </aside>
          </section>

          <section>
            <h1>Rackspace<br>Cloud Load Balancers</h1>
            <aside class="notes">
              Lekha: Rackspace Cloud Load Balancers for adding and removing servers to
              load balancers as they are created or deleted.
            </aside>
          </section>

        <section>
          <h2>Rackspace Identity</h2>
          <h3> is API compatible with </h3>
          <h2 style="color:orange" class="fragment grow"> Openstack Identity v2 </h2>
          <aside class="notes">
            Lekha: Rackspace Identity is API-compatible with (CLICK) OpenStack
            Identity v2.

          </aside>
        </section>

        <section>
          <h2>Rackspace Cloud Servers</h2>
          <h3> is powered by </h3>
          <h2 style="color:orange" class="fragment grow"> Openstack Compute </h2>
          <aside class="notes">
            Lekha: Rackspace Cloud Servers is powered by (CLICK) OpenStack Compute.

          </aside>
        </section>

        <section>
          <h2>Rackspace Cloud Load Balancers</h2>
          <h3> is a </h3>
          <h2 style="color:orange" class="fragment shrink"> Custom API </h2>
          <aside class="notes">
          <aside class="notes">
            Lekha: Although Rackspace Cloud Load Balancers is a custom API.
          </aside>
        </section>

        <section>
          <h1>Testing</h1>
          <h2 class="deemphasized">(for Auto Scale)</h2>
          <aside class="notes">
            Lekha: As Auto Scale was interacting with so many other systems,
            testing Auto Scale did not mean <b>just</b> testing features of
            Auto Scale. But also that, if any of these systems it depended on
            did not behave as expected, Auto Scale did not crumble and crash,
            but was consistent and able to handle these upstream failures
            gracefully.
            <br>
            So, there were two kinds tests for Auto Scale:

          </aside>
        </section>

        <section>
          <h1>Functional</h1>
          <h2>(API contracts)</h2>
          <aside class="notes">
            Lekha: One, was the Functional tests to validate the API
            contracts. These tests verified the responses of the Auto Scale
            API calls given valid, or malformed requests.

          </aside>
        </section>

        <section>
          <h1>System Integration</h1>
          <br>
          <br>

          <center>
            <table>
              <tr>
                <td>
                  <h2>Auto Scale</h2>
                </td>
                <td>
                  <h2>→</h2>
                </td>
                <td>
                  <h2>Identity</h2>
                </td>
              </tr>
              <tr>
                <td>
                </td>
                <td>
                  <h2>→</h2>
                </td>
                <td>
                  <h2>Compute</h2>
                </td>
              </tr>
              <tr>
                <td>
                </td>
                <td>
                  <h2>→</h2>
                </td>
                <td>
                  <h2>Load Balancers</h2>
                </td>
              </tr>
            </table>
          </center>

          <aside class="notes">
            Lekha: And the other was the System integration tests. Theese,
            were more complex. These tests verified integration between Auto
            Scale and Identity, Compute, and Load balancers.

          </aside>
        </section>

        <section>
          <h1>System Integration</h1>
          <h2 style="color:rgb(0,255,0)" class="fragment fade-out">Success</h2>
          <h2 style="color:rgb(255,0,0)" class="fragment current-visible">Failure</h2>

          <aside class="notes">
            Lekha: For example: When a scaling group was created, one such
            test will verify that the servers on that group were provisioned
            successfully. That is, that the servers went into an 'active'
            state and were then added as a nodes to the load balancer on the
            scaling group.(DOUBLE CLICK)<br>
            Or, if a server went into an error state, (yes! that can
            happen!), Auto Scale was able to re-provision that server
            successfully, and then add that active server to the load balancer
            on the scaling group.

          </aside>
        </section>

        <section>
          <h1>Testing Problems</h1>
          <h2 class="fragment">Test run time <span style="color: orange"> &ge;</span> server build time</h2>
          <aside class="notes">
            Lekha: All these tests were set up to run against the real
            services. And... here are some observations I had whilst writing
            the tests: (CLICK)<br>
            Servers could take over a minute, or ten minutes, or
            longer to provision. And the tests would run that-much-longer.
          </aside>
        </section>

        <section class="replace">
          <h1> <span style="color:orange"> BUILD → <span style="display:inline-block;"><span style="color:rgb(0,255,0)" class="fragment current-visible">ACTIVE </span> <span style="color:rgb(255,0,0)" class="fragment current-visible">ERROR </span><span style="visibility:hidden">ACTIVE</span></span></h1>
          <aside class="notes">
            Lekha: Sometimes, the tests would fail! due to raandom upstream failures.
            Like a test would expect a building server to go into an 'active' state,
            but it would (CLICK) go into an ERROR state
          </aside>
        </section>

        <section>
          <h1> No way to test</h1>
          <h1> unknown errors</h1>
          <aside class="notes">
            Lekha: And tests for such negative scenarios, like actually testing how
            Auto Scale would behave if the server did go into 'error' state,
            could not be tested. This is something that could <b>not</b> be
            reproduced consistently.
          </aside>
        </section>


        <section>
          <h1>However...</h1>
          <h2 class="fragment">Improving test coverage</h2>
          <h2 class="fragment">Tests → gate</h2>
          <aside class="notes">
            Lekha: However, (CLICK)the overall test coverage was improving. And I
            continued to add tests, oblivious of the time it was taking run
            the entire test suite!
            <br>
            Later, (CLICK) we had started using these tests as a <em>gate</em>, in the
            Autoscale merge pipeline.
          </aside>
        </section>


        <section>
          <h1> And... </h1>
          <h2 class="fragment"> Slow, flaky tests</h2>
          <h2 class="fragment"> Unhappy peers</h2>
          <aside class="notes">
            Lekha: And, (CLICK) the tests were running for <b>so</b> long and were sometimes flaky.
            Nobody dared to run these tests locally! Not even me, when I was
            adding more tests! (CLICK)
            <br>
            Also, our peers from the compute and load balancers teams, whose
            resources we were using up for our "Auto-scale" testing, were
            <em>not</em> happy! So much, so that, we were <em>pretty</em> glad, we were in a
            remote office!

          </aside>
        </section>

        <section>
          <h1>We've Had Enough!</h1>
          <h2 class="deemphasized">(on Auto Scale)</h2>
          <aside class="notes">
            Lekha: But! We had had enough! This had to change! we needed
            something! to save us from these slow flaky tests!

          </aside>
        </section>

        <section>
          <h2>There And Back Again<br><br></h2>
          <center>
            <table class="taba">
              <tbody class="fragment current-visible">
                <tr><td><h1>Specific</h1></td> <td><h1>→</h1></td> <td><h1>General</h1></td></tr>
                <tr class="deemphasized"><td>Auto Scale</td> <td></td> <td>Mimic</td></tr>
              </tbody>
              <tbody class="fragment current-visible">
                <tr><td><h1>General</h1></td> <td><h1>→</h1></td>
                  <td><h1>Specific</h1></td>
                <tr class="deemphasized"><td>Mocking Failure</td> <td></td> <td>Mimic Mimicking OpenStack</td></tr>
              </tbody>
            </table>
          </center>
          <aside class="notes">
            Glyph: Now that we've had enough, how are we going to solve this
            problem?  (CLICK) Since we've been proceeding from the specific case of
            Auto Scale to the general utility of Mimic, (click) let's go back to
            the general and proceed to the specific.
          </aside>
        </section>

        <section>
          <h1>General →</h1>
          <h2 class="fragment visible">Testing For Failure</h2>

          <aside class="notes">
            Glyph: When you have a complex distributed system - that is, a
            service A which makes requests of another service B (click) - you
            have to deal with the whole spectrum of real failures which might
            come back from service B.
          </aside>
        </section>
        <section>
          <h1>Real Services</h1>
          <h1 style="font-size: 9em; color: red;">FAIL</h1>
          <aside class="notes">
            Glyph: That service, B, is definitely going to fail, so you need to
            handle that.  However, if you always test against a real service,
            that real service might just be down, making it impossible to test
            your success cases until it is available again; failing more than
            is useful for your testing.
          </aside>
        </section>

        <section>
          <h1>But <span style="color: rgb(0, 255, 0)">Not</span> When You</h1>
          <h1 style="font-size: 6em; color: rgb(0, 255, 0);">WANT THEM TO</h1>
          <aside class="notes">
            Glyph: And when you do write the code to handle errors that come
            back from a service, you need to handle a wide variety of error
            cases.  Those errors won't come back from the real service every
            time.  So you won't be testing those very important code paths,
            most of the time.
          </aside>
        </section>

        <section>
          <h1>Succeeding</h1>
          <h2>At Success</h2>
          <aside class="notes">
            Glyph: Your positive-path code - the code that submits a request
            and gets the response that it expects - is going to get lots of
            testing in the real world.  Most interactions with services are
            successful, and the operators of those services always strive to
            make ever more of those interactions successful.  So most likely,
            the positive-path code is going to get exercised all the time and
            you will have plenty of opportunities to flush out bugs.
          </aside>
        </section>
        <section>
          <h1>Means Failing</h1>
          <h2>At Failure</h2>
          <aside class="notes">
            Glyph: If you test against real services, your negative-path code,
            will only get invoked in production when there's a real error.  If
            everything is going as planned, this should be infrequent.
          </aside>
        </section>
        <section>
          <h1>Mimic Succeeds</h1>
          <h2>At Failure!</h2>
          <aside class="notes">
            Glyph: It's really important to get negative-path code right.  If
            everything is going well, it's probably okay if your code has a
            couple of bugs.  You might be able to manually work around them.
          </aside>
        </section>
        <section>
          <h1 style="font-size: 800%">😈 ☁</h1>
          <h3>(Production)</h3>
          <aside class="notes">
            Glyph: But if things are starting to fail with some regularity in
            your cloud - that is to say - if you are using a cloud - that is
            exactly the time you want to make sure <em>your</em> system is
            behaving correctly: accurately reporting the errors, measuring the
            statistics, and allowing you to stay on top of incident management
            for your service.
          </aside>
        </section>
        <section>
          <h1 style="font-size: 800%">😇 ☁</h1>
          <h3>(Staging)</h3>
          <aside class="notes">
            Glyph: When you test against a real service, you are probably
            testing against a staging instance.  And, if your staging instance
            is typical, it probably doesn't have as much hardware, or as many
            concurrent users, as your production environment.  Every additional
            piece of harware or concurrent user is another opportunity for
            failure, so that means your staging environment is probably even
            less likely to fail.
          </aside>
        </section>
        <section>
          <pre style="border: 1px solid white; font-size: 70px;"><code class="language-python">import unittest</code></pre>
          <aside class="notes">
            Glyph: I've been in the software industry for long enough now to
            remember where this part of the talk would be the hardest part -
            the part where we try to sell the idea that code coverage and
            automated testing is really important.  Luckily we have moved on
            from the bad old days of the 90s, when most teams didn't have build
            automation, and if they wanted it they might not even be able to
            afford it.
          </aside>
        </section>
        <section>
          <h2 style="text-transform: none;"><code>test_stuff ... <span style="color: rgb(0, 255, 0);">[OK]</span></code></h2>
          <aside class="notes">
            Glyph: Luckily today we are all somewhat more enlightened, and we
            know that testing is important and full code coverage is important.
            So when we write code like this:
          </aside>
        </section>
        <section>
          <pre style="font-size: 40px"><code class="language-python" style="padding: 1em;">try:
    result = service_request()
except:
    return error
else:
    return ok(result)</code></pre>
          <aside class="notes">
            Glyph: ... we know that we need to write tests for this part:
          </aside>
        </section>
        <section>
<pre style="font-size: 40px"><span class="codelike" style="padding: 1em;"><span class="keyword">try</span>:
    result = service_request()
<div class="highlighted"><span class="keyword">except</span>:
    <span class="keyword">return</span> error
</div><span class="keyword">else</span>:
    <span class="keyword">return</span> ok(result)</span></pre>
          <aside class="notes">
            Glyph: ... and one popular way to get test coverage for those error
            lines is by writing a custom mock for it.
          </aside>
        </section>
        <section>
          <img src="Alice_par_John_Tenniel_34_Cropped.png" class="aliceimg">
          <aside class="notes">
            Glyph: The problem with the traditional definition of a mock is that each
            mock is defined just for the specific test that it's mocking.
          </aside>
        </section>
        <section>
          <img src="Alice_par_John_Tenniel_34_Cropped_Tears.png" class="aliceimg">
          <aside class="notes">
            Glyph: The problem with the traditional definition of a mock is
            that each mock is defined just for the specific test that it's
            mocking.  There's a reason that the Mock Turtle is crying.  Mocking
            is often a sad enterprise.
            <p>
              Let's take a specific example from OpenStack Compute.
            </p>
          </aside>
        </section>
        <section>
          <pre style="font-size: 24pt;"><code class="language-python">if not os.chdir(ca_folder(project_id)):
    raise exception.ProjectNotFound(
        project_id=project_id)</code></pre>
          <aside class="notes">
            Glyph: In June of this year, OpenStack
              Compute <a href="https://github.com/openstack/nova/commit/1a6d32b9690b4bff709dc83bcf4c2d3a65fd7c3e">introduced
              a bug</a> making it impossible to revoke a certificate.  The
              lines of code at fault were these two additions here.

            This is not a criticism of Nova itself;
            <a href="https://github.com/openstack/nova/commit/96b39341d5a6ea91d825d979e2381b9949b26e27">the
              bug was later fixed</a>.
          </aside>
        </section>
        <section>
          <pre style="font-size: 24pt;"><span class="codelike language-python"><span class="highlighted"><span class="keyword">if</span> <span class="keyword">not</span> os.chdir</span>(ca_folder(project_id)):
    <span class="keyword">raise</span> exception.ProjectNotFound(
        project_id=project_id)</span></pre>
          <aside class="notes">
            Glyph: The bug here is that <code>chdir</code> does not actually
            return a value.
          </aside>
        </section>
        <section>
          <pre style="font-size: 17pt"><code class="language-python">@mock.patch.object(os, 'chdir', return_value=True)
def test_revoke_cert_process_execution_error(self):
    "..."

@mock.patch.object(os, 'chdir', return_value=False)
def test_revoke_cert_project_not_found_chdir_fails(self):
    "..."</code></pre>
          <aside class="notes">
            Glyph: Because these tests construct their own mocks
            for <code>chdir</code>, we properly cover all the code, but the
            code is not integrated with a system that is verified in any way
            against what the real system (in this case, Python's chdir) does.
          </aside>
        </section>
        <section>
          <img src="folders.png" />
          <aside class="notes">
            Glyph: In this <em>specific</em> case, we might have simply tested
            against a real directory structure in the file system, because
            relative to the value of testing against a real implementation,
            creating a directory is not a terribly expensive operation.
          </aside>
        </section>
        <section>
          <img src="openstack_havana_conceptual_arch.png" />
          <aside class="notes">
            Glyph: However, standing up an OpenStack cloud is a significantly
            more work than making a new folder.  If you are developing an
            application that depends on OpenStack, creating a real cloud to
            test against is far too expensive and slow as Auto Scale's
            experience shows.  Creating a one-off mock for every test is fast
            to get started with and fast to run, but is error prone and
            rapidily becomes a significant maintainance burden of its own
            right. Auto Scale needed something that was quick to deploy, and
            lightweight to run like a mock, but realistic and usable in an
            integration scenario like a __real__ system.
          </aside>
        </section>
        <section>
          <h1>→Specific</h1>
          <h2 class="fragment visible">Mimic</h2>
          <aside class="notes">
            Glyph: Since we've been proceeding from the general to the
            specific, right here, where we need a realistic mock of a back-end
            openstack service, is where the specific value of Mimic comes in.
          </aside>
        </section>
        <section>
          <h1>Mimic</h1>
          <h2>Version 0.0</h2>
          <aside class="notes">
            Lekha: The first version of Mimic was built as a stand-in service
            for Identity, Compute and Rackspace Load balancers, the services
            that Auto Scale depends on.
          </aside>
        </section>
        <section>
          <h1>Pretending</h1>
          <h2 style="visibility: hidden">...</h2>
          <aside class="notes">
            Lekha: The essence of Mimic is pretending. The first thing that you
            must do to interact with it is to...
          </aside>
        </section>
        <section>
          <h1>Pretending</h1>
          <h2>to authenticate</h2>
          <aside class="notes">
            Lekha: ...pretend to authenticate.
            <br>
            Mimic does not validate credentials - all authentications will
            succeed. As with the real Identity endpoint, Mimic's identity
            endpoint has a service catalog which includes URLs for all the
            services implemented within Mimic. <br>
            A well behaved OpenStack client will use the service catalog to
            look up URLs for its service endpoints. Such a client will only
            need two pieces of configuration to begin communicating with the
            cloud, i.e. credentials and the identity endpoint. A client
            written this way will only need to change the Identity endpoint to
            be that of Mimic.

          </aside>
        </section>
        <section>
          <h1>Pretending</h1>
          <h2>to Boot Servers</h2>
          <aside class="notes">
            Lekha: When you ask Mimic to create a server, it pretends to create
            one.  This is not like stubbing with static responses: when Mimic
            pretends to build a server, it remembers the information about that
            server and will tell you about it in the subsequent requests.
          </aside>
        </section>
        <section>
          <h1>Pretending</h1>
          <h2>is <em>faster</em></h2>
          <aside class="notes">
            Lekha: Mimic was originally created to speed things up. So, it was
            very important that - it be fast both to respond to requests, and to
            have developers setup.

          </aside>
        </section>

        <section>
          <h1>in-memory</h1>
          <aside class="notes">
            Lekha: It was built using in-memory data structures.
          </aside>
        </section>

        <section>
          <h1>minimal software dependencies</h1>
          <p text-transform:lowercase> (almost entirely pure Python) </p>
          <aside class="notes">
            Lekha: with minimal software dependencies, almost entirely pure Python.
          </aside>
        </section>

        <section>
          <h1>
            <span style='color:black;text-decoration:line-through'>
              <span style='color:white'>Service Dependencies</span>
            </span>
          </h1>
          <aside class="notes">
            Lekha: With no service dependencies
          </aside>
        </section>

        <section>
          <h1>
          <span style='color:black;text-decoration:line-through'>
              <span style='color:white'>Configuration</span>
            </span>
          </h1>
          <aside class="notes">
            Lekha: and no configuration
          </aside>
        </section>

        <section>
          <h1>self-contained</h1>
          <aside class="notes">
            Lekha: And is entirely self-contained.
          </aside>
        </section>

        <section>
          <h1>Demo!</h1>
          <h2 style="color: orange"> Nova comand-line client <h2>
          <aside class="notes">
            Lekha: Lets see how we can run the python nova command-line client against Mimic
          </aside>
        </section>


        <section>
          <h1>config.sh</h1>
  <pre style="font-size: 23px"><span class="codelike" style="padding: 1em;"><span class="keyword">export</span> OS_USERNAME=username
<span class="keyword">export</span> OS_PASSWORD=password
<span class="keyword">export</span> OS_TENANT_NAME=11111
<span class="keyword">export</span> OS_AUTH_URL=http://localhost:8900/identity/v2.0/tokens</pre>
          <aside class="notes">
            Lekha: Here is the config file that holds
            the  environment variables required for the OpenStack 
            command-line clients. 
          </aside>
        </section>
        <section>
          <h1>config.sh</h1>
  <pre style="font-size: 23px"><span class="codelike" style="padding: 1em;"><span class="highlighted"><span class="keyword">export</span> OS_USERNAME=username</span>
<span class="highlighted"><span class="keyword">export</span> OS_PASSWORD=password</span>
<span class="highlighted"><span class="keyword">export</span> OS_TENANT_NAME=11111</span>
<span class="keyword">export</span> OS_AUTH_URL=http://localhost:8900/identity/v2.0/tokens</pre>
          <aside class="notes">
            Lekha: We have set a random username, password
            and tenant name, as Mimic only <em>pretends</em> to authenticate

          </aside>
        </section>
        <section>
          <h1>config.sh</h1>
  <pre style="font-size: 23px"><span class="codelike" style="padding: 1em;"><span class="keyword">export</span> OS_USERNAME=username
<span class="keyword">export</span> OS_PASSWORD=password
<span class="keyword">export</span> OS_TENANT_NAME=11111
<span class="highlighted"><span class="keyword">export</span> OS_AUTH_URL=http://localhost:8900/identity/v2.0/tokens</span></pre>
          <aside class="notes">
            Lekha: And the Auth url is set to be that of Mimic.
            <br>

            Now, let's continue where we left off with our first demo. So we
            already have an instance of mimic running.
          
          </aside>
        </section>

        <section>
          <video src="python-client-demo.mp4">
          <aside class="notes">

            Lekha: Let's pip install the python nova-client and ensure the
            config file has the AUTH_URL pointing to that of Mimic. We source
            the config file and we see that no servers exist on Mimic start up! Let's
            create a server with a random flavor and image. The server created
            is in an <b> active </b> state. Lets create a second server, which
            also is built immediately and is an active state. Now we have 2
            active servers that Mimic knows of. Lets delete the second
            server... and now Mimic knows of the deleted server and has only
            the one server remaining.


          </aside>
        </section>

        <section>
          <h1>Using Mimic</h1>
          <h2>(on Auto Scale)</h2>
          <aside class="notes">

            Lekha: We did the same thing with Auto Scale. We pointed the tests
            and the Autoscale API to run against an instance of Mimic.

          </aside>
        </section>


        <section>
          <h1>The Results!</h1>
          <h2>(Functional tests using Mimic)</h2>
          <aside class="notes">

            Lekha: This reduced the test time exponentially! 
            Before Mimic the functional tests would take...

          </aside>
        </section>

        <section>
          <h3>Functional Tests:</h3>
          <h1 class="fragment grow" style="color:orange">15 minutes</h1>
          <h3>against a real system</h3>
          <h2>vs.</h2>
          <h1 class="fragment grow" style="color:orange">30 seconds</h1>
          <h3>against Mimic</h3>
          <aside class="notes">

            Lekha: (CLICK) 15 minutes to complete, and now they run in (CLICK)less than 30
            seconds!

          </aside>
        </section>

        <section>
          <h1>The Results!</h1>
          <h2>(System Integration tests using Mimic)</h2>
          <aside class="notes">

            Lekha: In the system integration tests, if one of the servers in
            the test remained in "building" for fifteen minutes longer than
            usual, then the tests ran fifteen minutes slower.

          </aside>
        </section>

        <section>
          <h3>System Integration Tests:</h3>
          <h1 class="fragment grow" style="color:orange">3 hours or more</h1>
          <h3>against a real system</h3>
          <h2>vs.</h2>
          <h1 class="fragment grow" style="color:orange">3 minutes</h1>
          <h3>against Mimic</h3>
          <aside class="notes">

            Lekha: These tests took (CLICK)<b>over 3 hours to complete</b> and using
            Mimic this went down to be (CLICK) less than 3 *minutes*
            <em>consistently</em>, to complete!

          </aside>
        </section>

        <section data-transition="linear"
                 data-transition-speed="slow">
          <h1 style="-webkit-transform: rotateY(180deg); font-size: 800px;">✈</h1>
          <aside class="notes">

            Lekha: All our dev VMs are now configured to run against
            Mimic. <br> One of our devs from the Rackspace Cloud Intelligence
            team, calls this "Developing on Airplane Mode!", as we can work
            offline without having to worry about uptimes of the upstream
            systems and get immediate feedback on the code being written.

          </aside>
        </section>
        <section>
          <h1> What about<br>error injection? </h1>
          <aside class="notes">
            Glyph: But Lekha, what about all the error injection stuff I
            mentioned before?  Does Mimic do that?  How did this set-up test
            Auto Scale's error conditions?
          </aside>
        </section>

        <section>
          <h1> Mimic does<br>simulate errors </h1>
          <aside class="notes">
            Lekha: Well Glyph, I am as pleased as I am suprised that you ask
            that.  Mimic does simulate errors.

          </aside>
        </section>

        <section>
          <h1>Error injection using <code>metadata</code></h1>
          <aside class="notes">
            Lekha: Earlier, when I said Mimic pretends to create a server, that
            wasn't entirely true - sometimes Mimic pretends to *not* create a
            server.  It uses the metadata provided during the creation of the
            server, inspects the metadata, and sets the state of the server
            respectively.Lets go back to the demo and see how this can be done.

          </aside>
        </section>

        <section>
          <video src="error-injection-demo.mp4"></video>
          <aside class="notes">

            Lekha:  So, we had the one active server. Now, lets  create a
            server with the `metadata`: `"server_building": 30`. This will
            keep the server in build state for 30 seconds. Now we have 2
            servers. The active and building sever.  Also, We can create a server
            that goes into an error state, using the `metadata`: `"server_error":
            True`. As you can see, we now have 3 different servers, with 3
            different states.

          </aside>
        </section>

        <section>
          <h1>Retry On Errors</h1>
          <aside class="notes">

            Lekha: For the purposes of Auto Scale it was important that we
            have right number of servers on a scaling group, even if a number
            of attempts to create one failed. We chose to use
            <code>metadata</code> for error injection so that requests with
            injected errors could also be run against real services. For Auto
            Scale, the expected end result is the same number of servers
            created, irrespective of the number of failures. But this behavior
            may also be useful to many other applications because retrying is
            a common pattern for handling errors.

          </aside>
        </section>

        <section>
          <h2>Mimic 0.0 was...</h2>
          <h1>Too Limited</h1>
          <aside class="notes">
            Lekha: However, the first implementation of mimic had some flaws,
            it was fairly Rackspace specific and only implemented the endpoints
            of the services that Autoscale depends upon . And they were all
            implemented as part of Mimic's core. It ran each service on a
            different port, meaning that for N endpoints you would need not
            just N port numbers, but N *consecutive* port numbers. It allowed
            for testing error scenarios, but only using the metadata. This was
            not useful for all cases, for example, for a control panel that
            does not not allow the user to enter any metadata.  <!-- (TODO: ask someone if Horizon matches this
            description?) -->

          </aside>
        </section>
        <section>
          <h2>Mimic 0.0 was...</h2>
          <h1>Single Region</h1>
          <aside class="notes">
            Lekha: Mimic also did not implement multiple regions.  It used
            global variables for storing all state, which meant that it was
            hard to add additional endpoints with different state in the same
            running mimic instance.
          </aside>
        </section>
        <section>
          <h2>Beyond Auto Scale:</h2>
          <h1>Refactoring Mimic</h1>
          <aside class="notes">
            Glyph: Mimic had an ambitious vision: to be a one-stop mock for all
            OpenStack and Rackspace services that needed fast integration
            testing.  However, its architecture at the time severely limited
            the ability of other teams to use it or contribute to it.  As Lekha
            mentioned, it was specific not only to Rackspace but to Auto Scale.
          </aside>
        </section>
        <section>
          <h1>YAGNI</h1>
          <aside class="notes">
            Glyph: On balance, Mimic was also extremely simple.  It followed
            the You Aren't Gonna Need It principle of extreme programming very
            well, and implemented just the bare minimum to satisfy its
            requirements, so there wasn't a whole lot of terrible code to throw
            out or much unnecessary complexity to eliminate.
          </aside>
        </section>
        <section>
          <h1>E(ITO)YAGNI</h1>
          <aside class="notes">
            Glyph: There is, however, a corrolary to YAGNI, which is
            E(ITO)YAGNI: Eventually, It Turns Out, You *Are* Going To Need It.
            As Mimic grew, other services within Rackspace wanted to make use
            of its functionality, and a couple of JSON response dictionaries in
            global variables were not going to cut it any more.
          </aside>
        </section>
        <section data-background="plug.jpg">
          <h1>Plugins!</h1>
          <aside class="notes">
            Glyph: So we created a plugin architecture.
          </aside>
        </section>
        <section>
          <h1>Identity</h1>
          <h2>Is the Entry Point</h2>
          <h3>(Not A Plugin)</h3>
          <aside class="notes">
            Glyph: Mimic's Identity endpoint is the top-level entry point to
            Mimic as a service.  Every other URL to a mock is available from
            within the service catalog.  As we were designing the plugin API,
            it was clear that this top-level Identity endpoint needed to be the
            core part of Mimic, and plug-ins would each add an entry for
            themselves to the service catalog.
          </aside>
        </section>
        <section>
          <code style="font-size: 48px; line-height: 1.4em;">
            <span class="fragment mimic-core-url">http://localhost:8900/mimicking/ NovaApi-78bc54/ORD/</span> <span class="fragment mimic-plugin-url">v2/tenant_id_f15c1028/servers</span>
          </code>
          <aside class="notes">
            Glyph: URLs within Mimic's service catalog all look similar.  In
            order to prevent conflicts between plugins, Mimic's core one
            encodes the name of your plugin and the region name specified by
            your plugin's endpoint.  Here we can see what a URL for the Compute
            mock looks like. (CLICK) This portion of the URL, which identifies
            which mock is being referenced, is handled by Mimic itself, so that
            it's always addressing the right plugin. (CLICK) Then there's the
            part of the URL that your plugin itself handles, which identifies
            the tenant and the endpoint within your API.
          </aside>
        </section>
        <section>
          <h1>Plugin Interface:<br>“API Mock”</h1>

          <aside class="notes">
            Glyph: Each plugin is an API mock, which has only two methods:
          </aside>
        </section>
        <section class="apimock">
          <pre><code class="language-python">class YourAPIMock():</code></pre>
          <pre class="fragment"><code class="language-python">  def catalog_entries(...)</code></pre>
          <pre class="fragment"><code class="language-python">  def resource_for_region(...)</code></pre>
          <br />
          <h3 class="fragment">(that's it!)</h3>

          <aside class="notes">
            Glyph: (click) <code>catalog_entries</code> (click) and <code>resource_for_region</code> (click) That's it!.
          </aside>
        </section>

        <section class="apimock">
          <pre><code class="language-python"
>def catalog_entries(self,
                    tenant_id):</code></pre>
          <aside class="notes">
            <p>
              Glyph: <code>catalog_entries</code> takes a tenant ID and returns the
              entries in Mimic's service catalog for that particular API mock.

            <p>
              APIs have catalog entries for each API type, which in turn have
              endpoints for each virtual region they represent.

          </aside>
        </section>
        <section>
<pre style="font-size: 18pt;"><span class="codelike language-python"><span class="keyword">return</span> [
    <span class="fragment highlight-current-green">Entry</span>(
        <span class="fragment highlight-current-green">tenant_id</span>, <span class="string"><span class="fragment highlight-current-green">"compute"</span></span>, <span class="string"><span class="fragment highlight-current-green">"cloudServersOpenStack"</span></span>,
        <span class="fragment highlight-current-green">[
            <span class="fragment highlight-current-green">Endpoint(tenant_id, region=<span class="string"><span class="fragment highlight-current-green">"ORD"</span></span>,
                     endpoint_id=text_type(uuid4()),
                     prefix=<span class="string"><span class="fragment highlight-current-green">"v2"</span></span>)</span>,
            Endpoint(tenant_id, region=<span class="string">"DFW"</span>,
                     endpoint_id=text_type(uuid4()),
                     prefix=<span class="string">"v2"</span>)
        ]</span>
    )
]
</span></pre>
          <aside class="notes">
            <p>
              Glyph: This takes the form of an iterable of a class called
              (CLICK) <code>Entry</code>, each of which is (CLICK) a tenant ID,
              (CLICK) a type, (CLICK) a name, (CLICK) and a collection of
              (CLICK) <code>Endpoint</code> objects, each (CLICK) containing (CLICK)
              the name of a pretend region, (CLICK) a URI version prefix that
              should appear in the service catalog after the generated service
              URL but before the tenant ID.
          </aside>
        </section>
        <section class="apimock">
<pre><span class="codelike language-python"><span class="function"><span class="keyword">def</span> <span class="title">resource_for_region</span><span class="params">(
    self, <span class="fragment highlight-current-green">region</span>, <span class="fragment highlight-current-green">uri_prefix</span>,
    <span class="fragment highlight-current-green">session_store</span>
)</span>:</span>
    <span class="fragment highlight-current-green"><span class="keyword">return</span> (YourRegion(...)
            .app.resource())</span>
</code></pre>
          <aside class="notes">
            Glyph: <code>resource_for_region</code> takes (CLICK) the name of a
            region, (CLICK) a URI prefix - produced by Mimic core to make URI
            for each service unique, so you can generate URLs to your services
            in any responses which need them - (CLICK) and a session store
            where the API mock may look up state of the resources it pretended
            to provision for the respective
            tenants. (CLICK) <code>resource_for_region</code> returns an HTTP resource
            associated with the top level of the given region.  This resource
            then routes this request to any tenant- specific resources
            associated with the full URL path.
          </aside>
        </section>
        <section>
          <pre style="font-size: 22pt;"><code class="language-python"
>class YourRegion():

    app = MimicApp()

    @app.route('/v2/&lt;string:tenant_id&gt;/servers',
               methods=['GET'])

    def list_servers(self, request, tenant_id):
        return json.dumps({"servers": []})
</code></pre>
          <aside class="notes">
            Glyph: Once you've created a resource for your region, it has a
            route for the parts of the URI that starts at the end of the URI
            path.  Here you can see what the nova "list servers" endpoint would
            look like using Mimic's API; as you can see, it's not a lot of work
            at all to return a canned response.  It would be a little beyond
            the scope of this brief talk to do a full tutorial of how resource
            traversal works in the web framework that Mimic uses, but hopefully
            this slide - which is a fully working response - shows that it
            is pretty easy to get started.
          </aside>
        </section>
        <section>
          <h1>Telling Mimic</h1>
          <h1>To Load It</h1>
        </section>
        <section>
          <pre style="font-size: 30pt;"><code class="language-python"
># mimic/plugins/your_plugin.py

from your_api import YourAPIMock
the_mock_plugin = YourAPIMock()
</code></pre>
          <aside class="notes">
            Glyph: To register your plugin with Mimic, you just need to drop an
            instance of it into any module of the <code>mimic.plugins</code>
            package.
          </aside>
        </section>
        <section>
          <h1>Mimic Remembers</h1>
          <h2 class="fragment">(until you restart it)</h2>
          <aside class="notes">
            Glyph: This, of course, just shows you how to create ephemeral,
            static responses - but as Lekha said previously, Mimic doesn't just
            create fake responses; it remembers - (CLICK) in memory - what
            you've asked it to do.
          </aside>
        </section>
        <section>
          <pre style="font-size: 26px;"><code class="language-python"
>session = session_store.session_for_tenant_id(tenant_id)

class YourMockData():
    "..."

your_data = session.data_for_api(your_api_mock,
                                 YourMockData)</code></pre>
          <aside class="notes">
            Glyph: That "session_store" object passed to resource_for_region is
            the place you can keep any relevant state.  It gives you a
            per-tenant session object, and then you can ask that session for
            any mock-specific data you want to store for that tenant.  All
            session data is created on demand, so you pass in a callable which
            will create your data if no data exists for that tentant/API pair.
          </aside>
        </section>
        <section>
          <pre style="font-size: 26px;"><code class="language-python"
>session = session_store.session_for_tenant_id(tenant_id)

from mimic.plugins.other_mock import (other_api_mock,
                                      OtherMockData)

other_data = session.data_for_api(other_api_mock,
                                  OtherMockData)</code></pre>
          <aside class="notes">
            Glyph: Note that you can pass other API mocks as well, so if you
            want to inspect a tenant's session state for other services and
            factor that into your responses, it's easy to do so.  This pattern
            of inspecting and manipulating a different mock's data can also be
            used to create control planes for your plugins, so that one plugin
            can tell another plugin how and when to fail by storing information
            about the future expected failure on its session.
          </aside>
        </section>
        <section>
          <h1>Errors As A Service</h1>
          <aside class="notes">
            Glyph: We are still working on the first error-injection endpoint
            that works this way, by having a second plugin tell the first what
            its failures are, but this is an aspect of Mimic's development we
            are really excited about, because that control plane API also
            doubles as a memory of the unexpected, and even potentially
            undocumented, ways in which the mocked service can fail.
          </aside>
          </aside>
        </section>
        <section>
          <h1>Error Conditions Repository</h1>
          <aside class="notes">
            Lekha: Anyone testing a product, will run into unexpected
            errors. Thats why we test!  But we dont know what we dont know, and
            cant be prepared for this ahead of time right!
          </aside>
        </section>
        <section>
          <h1>Discovering Errors</h1>
          <h2>Against Real Services</h2>
          <aside class="notes">
            Lekha: When we were running the Auto Scale tests against Compute,
            we began to see some one-off errors. Like, when provisioning a
            server, the test expected a server to go into a building state for
            some time before it is active, __but__ it would remain in building
            state for over an hour or even would sometimes go into an error
            state, after.
          </aside>
        </section>
        <section>
          <h1>Record Those Errors</h1>
          <h2>Within Mimic</h2>
          <aside class="notes">
            Lekha: Auto Scale had to handle such scenarios gracefully and the
            code was changed to do so. And Mimic provided for a way to tests
            this consistently.
          </aside>
        </section>
        <section>
          <h1>Discover More Errors</h1>
          <h2>Against Real Services</h2>
          <aside class="notes">
            Lekha: However, like I said, we dont know what we dont know. We
            were not anticipating on finding any other such errors. But, there were more!
            And this was slow process for us to uncover such errors, as we
            tested against the real services. 
          </aside>
        </section>

        <section>
          <h1>Record Those Errors</h1>
          <h2>For The Next Project</h2>
          <aside class="notes">

            Lekha: And, we continued to add such errors to Mimic. Now, Wont it be great if not every client that depended on
            a service had to go through this same cycle. Not everyone
            had to find all the possible error conditions in the service <b>by
            experience</b>. And have to deal with them at the pace that they
            occur.
          
          </aside>
        </section>

        <section>
          <h1>Share A Repository</h1>
          <h2>For Future Projects</h2>
          <aside class="notes">
            Lekha: What if we had a repository of all such known errors, that
            everyone contributes to. So the next person using the plugin can
            use the existing ones, and ensure there application behaves
            consistently irrespective of the errors. And be able add any new
            ones to it.
          </aside>
        </section>
        <section>
          <h1>Mimic Is A Repository</h1>
          <aside class="notes">
            Lekha: Mimic is just that, a repository of all known responses
            including the error responses.
          </aside>
        </section>

        <section>
          <h1>Mimic Endpoint</h1>
          <br>
          <code style="font-size: 50px">/mimic/v1.0/presets</code></pre>
          <aside class="notes">

            Lekha: Mimic has an endpoint `presets` that lists all
            the errors conditions that can be simulated using Mimic.
          
          </aside>
        </section>

        <section>
          <h1>Control</h1>
          <aside class="notes">
            Glyph: In addition to storing a repository of errors, Mimic allows
            for finer control of behavior beyond simple success and error.  You
            can determine the behavior of a mimicked service in some detail.
          </aside>
        </section>

        <section>
          <h1>Now &amp; Later</h1>
          <aside class="notes">
            Glyph: We're not just here today to talk about exactly what Mimic
            offers right now, but where we'd like it to go.  And in that spirit
            I will discuss one feature that Mimic has for controlling behavior
            today, and one which we would like to have in the future.
          </aside>
        </section>

        <section>
          <h1>Now</h1>
          <aside class="notes">
            Glyph: Appropriately enough, since I'm talking about things now and
            things in the future, the behavior-control feature I'd like to talk
            about that that Mimic has right now is the ability to control time.
          </aside>
        </section>
        <section>
          <h1><code>now()</code></h1>
          <aside class="notes">
            Glyph: That is to say: when you do something against Mimic that
            will take some time, such as building a server, time does not
            actually pass ... for the purposes of that operation.
          </aside>
        </section>
        <section>
          <h2><code>/mimic/v1.1/tick</code></h2>
          <aside class="notes">
            Glyph: Instead of simply waiting 10 seconds, you can hit this
            second out-of-band endpoint, the "tick" endpoint ...
          </aside>
        </section>
        <section class="apimock">
          <pre><code class="language-javascript"
>{
    "amount": 100.0
}
</code></pre>
          <aside class="notes">
            Glyph: with a payload like this.  It will tell you that time has
            passed, like so:
          </aside>
        </section>
        <section>
          <pre style="font-size: 34px"><code class="language-javascript"
>{
    "advanced": 1.0,
    "now": "1970-01-01T00:00:01.000000Z"
}</code></pre>
          <aside class="notes">
            Glyph: with a payload like this.  It will tell you that time has
            passed, like so.  Now, you may notice there's something a little
            funny about that timestamp - it's suspiciously close to midnight,
            january first, 1970.  Mimic begins each subsequent restart thinking
            it's 1970, at the unix epoch; if you want to advance the clock,
            just plug in the number of seconds since the epoch as the "amount"
            and your mimic will appear to catch up to real time.
          </aside>
        </section>
        <section>
          <pre style="font-size: 32px"><code class="language-javascript"
>{
  "server": {
    "status": "BUILD",
    "updated": "1970-01-01T00:00:00.000000Z",
    "OS-EXT-STS:task_state": null,
    "user_id": "170454",
    "addresses": {},
    "...": "..."
  }
}</code></pre>
          <aside class="notes">
            Glyph: If you've previously created a server with "server_building"
            metadata that tells it to build for some number of seconds, and you
            hit the 'tick' endpoint telling it to advance time the
            server_building number of seconds...
          </aside>
        </section>
        <section>
          <pre style="font-size: 32px"><code class="language-javascript"
>{
  "server": {
    "status": "ACTIVE",
    "updated": "1970-01-01T00:00:01.000000Z",
    "OS-EXT-STS:task_state": null,
    "user_id": "170454",
    "addresses": {},
    "...": "..."
  }
}</code></pre>
          <aside class="notes">
            Glyph: that server (and any others) will now show up as "active",
            as it should.  This means you can set up very long timeouts, and
            have servers behave "realistically", but in a way where you can
            test several hours of timeouts a time.
          </aside>
        </section>

        <section>
          <h1>Later</h1>
          <aside class="notes">
            Glyph: Another feature that isn't implemented yet, that we hope to
            design later, is the ability to inject errors ahead of time, using
            a separate control-plane interface which is not part of a mock's
            endpoint.
          </aside>
        </section>

        <section>
          <h1> Error Injection </h1>
          <h2> </h2>
          <aside class="notes">
            Glyph: We've begun work on a branch doing this for Compute, but we
            feel that every service should have the ability to inject arbitrary
            errors.
          </aside>
        </section>

        <section>
          <h1> Error Injection </h1>
          <h2>Currently: Metadata-Based</h2>
          <aside class="notes">
            Glyph: As Lekha explained, Mimic can already inject some errors by
            supplying metadata within a request itself.
          </aside>
        </section>

        <section>
          <h1> Error Injection </h1>
          <h2>Currently: In-Band</h2>
          <aside class="notes">
            Glyph: However, this means that in order to cause an error to
            happen, you need to modify the request that you're making to mimic,
            which means your application isn't <em>entirely</em> unmodified.
          </aside>
        </section>

        <section>
          <h1> Error Injection </h1>
          <h2>Future: Separate Service Catalog Entry</h2>
          <aside class="notes">
            Glyph: What we'd like to do in the future is to put the
            error-injection control plane into the service catalog, with a
            special entry type so that your testing infrastructure can talk to
            it.
          </aside>
        </section>

        <section>
          <h1> Error Injection </h1>
          <h2>Future: Out-Of-Band</h2>
          <aside class="notes">
            Glyph: This way, your testing tool would authenticate to mimic, and
            tell Mimic to cause certain upcoming requests to succeed or fail
            before the system that you're testing even communicates with
            it. Your system would not need to relay any expected-failure data
            itself, and so no metadata would need to be passed through.
          </aside>
        </section>

        <section>
          <h1>Error Injection</h1>
          <h2>Future: With Your Help</h2>
          <aside class="notes">
            Glyph: What we'd really like to build with these out-of-band
            failures, though, is not just a single feature, but an API that
            allows people developing applications against openstack to make
            those applications as robust as possible by easily determining how
            they will react at scale, under load, and under stress, even if
            they've never experienced those conditions.  So we need you to
            contribute the errors and behaviors that <em>you</em> have
            experienced.
          </aside>
        </section>

        <section>
          <h1>Even Later...</h1>
          <aside class="notes">
            Glyph: Mimic is based on a networking framework ...
          </aside>
        </section>

        <section>
          <img src="gruehead.png" class="grue">
          <aside class="notes">
            Glyph: ... some of you know which one I'm talking about ...
          </aside>
        </section>

        <section>
          <h1>Even Later...</h1>
          <h2>Future Possibilities,<br>Crazy Features!</h2>
          <aside class="notes">
            Glyph: ... which has such features as built-in DNS and SSH servers.
          </aside>
        </section>

        <section>
          <h2>Even Later...</h2>
          <h1>Real <span class="contactlabel">SSH</span> Server</h1>
          <h2 class="deemphasized">For Fake Servers</h2>
          <aside class="notes">
            Glyph: It would be really cool if when a virtual server was booted,
            the advertised SSH port really did give you access to an SSH
            server, albeit one that can be cheaply created from a local shell
            as a restricted user or a container deployment, not a real virtual
            machine.
          </aside>
        </section>

        <section>
          <h2>Even Later...</h2>
          <h1>Real <span class="contactinfo">DNS</span> Server</h1>
          <h2 class="deemphasized">For Fake Zones</h2>
          <aside class="notes">
            Glyph: Similarly, if we were to have a Desginate mock, it would be
            really cool to have real DNS entries.
          </aside>
        </section>

        <section>
          <h1>Mimic for OpenStack</h1>
          <aside class="notes">
            Lekha: Mimic, can be the tool, where you do not have to stand up
            the entire dev stack to understand how an OpenStack API behaves.
            <p>
              Mimic can be the tool which enables an OpenStack developer to get
              quick feedback on the code he/she is writing and not have to go
              through the gate multiple times to understand that - "maybe I
              should have handled that one error, that the upstream system
              decides to throw my way every now and then"
          </aside>
        </section>

        <section>
          <h1>It's Easy!</h1>
          <aside class="notes">
            Glyph: One of the things that I like to point out is that Mimic is
            not real software.  It's tiny, self-contained, doesn't need to
            interact with a database, or any external services.  Since it
            mimics exclusively existing APIs, there are very few design
            decisions.  As a result, contributing to Mimic is a lot easier than
            contributing to OpenStack proper.
          </aside>
        </section>

        <section>
          <h1>We need your help!</h1>
          <center>
            <table>
              <tr>
                <td class="contactlabel">Source:</td> <td class="contactinfo"><a href="https://github.com/rackerlabs/mimic">https://github.com/rackerlabs/mimic</a></td>
              </tr>
              <tr>
                <td class="contactlabel">Issues:</td> <td class="contactinfo"><a href="https://github.com/rackerlabs/mimic">https://github.com/rackerlabs/mimic/issues</a></td>
              </tr>
              <tr>
                <td class="contactlabel">Chat:</td> <td class="contactinfo"><a href="irc://chat.freenode.net/##mimic">##mimic on Freenode</a></td>
              </tr>
            </table>
          </center>
          <aside class="notes">

            Lekha: So, please come join us build Mimic. Together we can make
            this a repository for all known reponses (including errors!) for the
            OpenStack APIs.  <br>

            As we mentioned earlier, Mimic is Opensource and here is the
            github link to the repository. 

            All the features or issues we are working on, or planning to work
            on, in the near future are under the issues tab on github.

            You can <em>start</em> by using Mimic and giving us your feedback. Or better
            yet, forking it and contributing to it, by adding plugins for
            services that do not exist today! 

            Thank you!

          </aside>
        </section>

      </div>

    </div>

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.min.js"></script>

    <script src="configuration.js"> </script>
    <script src="custom.js"></script>

  </body>
</html>
